%!TEX root = thesis.tex

\chapter{Gesture Recognition}
\label{ch:gestures}

Knowing the location of the hands in a image doesn't tell much about the hand poses. A method needs to be constructed to discriminate the different hand poses.

This chapter describes how the hand poses are extracted from the hand windows. First features of the hand window are extracted that represent the current hand pose. These features are compared by a classifier with previously extracted features of hand poses for which the poses where already known. The classifier will determine which known hand pose(s) resembles the at that moment classified hand.


\section{Feature extraction}
To compare two or more hands with each other a method is required to calculate the similarity. A very simple method is to compare each pixel location with each other and sum all the differences. But this method isn't feasible. First of all, it is not going to work on images that are not the same size or orientation. Also it is computationally expensive for larger images - for an image of 100 by 100 pixels there are already 10.000 dimensions. And a other important thing, this method is not very forgiving for small variations like scaling and rotation.


BRIDGE

\subsection*{Segmentation - removing background}

\begin{figure}[htbp]
\begin{center}
\subfloat[Hand window]{
        \includegraphics[width=0.2\linewidth]{figures/pipeline/lefthand.jpg}
        \label{fig:lefthandwindow}
}
\hspace{0.03\linewidth}
\subfloat[Hand cutout]{
        \includegraphics[width=0.2\linewidth]{figures/pipeline/lefthandcutout.jpg}
        \label{fig:lefthandcutout}
}
\hspace{0.03\linewidth}
\subfloat[Hand features]{
        \includegraphics[width=0.2\linewidth]{figures/pipeline/lefthandhog.jpg}
        \label{fig:lefthandfeatures}
}
\end{center}
\end{figure}

INVERSE SKIN MASK

The hand window contains some background pixels, because a hand will never fill a perfect square. These pixels are unwanted since they contain arbitrary values that introduce noise into our process. In \autoref{sec:skinmodel} a binary mask for skin pixels is constructed. The binary inversion of this mask can be used again to remove the background.

Sometimes the blob is too small to say something useful about the hand pose. One way to artificially increase the blob size is to do a morphological dilate operation on the blob. This will increase the size of the hand cutout and probably add skin pixels, but this will also introduce more noisy (non-skin) pixels.

\subsection*{Descriptors}
To reduce the number of dimensions of an image a descriptor can be used. A descriptor describes an image in, if properly used, less dimensions than the image itself. Also it can introduce some useful properties like scale and rotation invariance. In this paper 2 descriptors are discussed and compared, Histogram of oriented gradient\cite{NavneetDalal2006} (HOG) and Speeded Up Robust Features\cite{Bay2006} (SURF).

The HOG descriptor has been successfully applied and studied in human detection \cite{NavneetDalal2006, watanabe2009}. The HOG descriptors method uses a dense grid of uniformly spaced cells, where for each cell the gradients are calculated in 9 orientations. The values for each orientation for each cell are stored in a histogram that represent the image. Also contrast normalization is applied for improved accuracy.

The SURF descriptor is based on sums of approximated 2D Haar wavelet responses and makes use of integral images. SURF is approximates the speed of SIFT and sometimes is even faster\cite{Murillo2007, Valgren2010}. Also it is claimed to be more robust against several image transformations.

To be able to compare the two descriptors better, the SURF descriptor is configured to work with a fixed set of interest points set up as a dense grid - the same set used for HOG. Using the interest point detection is a expensive operation, calculating this on a small image takes about 500 ms.

For the experiments described in \autoref{ch:experiments} the same parameters as in the \cite{watanabe2009} paper are used, except that the image is not resized to 64 by 128 pixels, but 128 by 128 pixels.



\section{Classification}
Using a set of training examples each marked as belonging to a category, a classifier builds a model that predicts whether a new example falls into one category or the other. 

There are many classifiers which can be used in different ways, for computer vision two of these are interesting because of simplicity or speed - k-NN and SVM.

\subsection*{k Nearest Neighbors}
The k-nearest neighbours algorithm (k-NN) is a method for classifying objects based on closest training examples in the feature space. The k-nearest neighbor algorithm is one of the the simplest of all machine learning  algorithms. K-NN is a type of 'lazy learning' where all computation is deferred until classification. This results in a fast training phase, but a computationally expensive classification. Also the memory footprint is large since all data points are stored. 

The training samples are labeled multidimensional vectors. The training phase of the algorithm consists only of storing the vectors and class labels of the training samples.

In the classification phase, $k$ is a user-defined constant, and an unlabeled vector is classified by assigning the label which is most frequent among the k training samples nearest to that query point.



\autoref{fig:knn} is an example of k-NN classification. The test sample (the circle) should be classified either to the squares class or to the triangles class. If $k = 3$ the circle is classified to the triangles class because there are 2 triangles and 1 square inside the inner circle. If $k = 5$ it is classified to squares class.

\begin{figure}[htbp]
\center{}
\includegraphics[width=0.3\linewidth]{figures/knn.png}
\caption{Classification using K Nearest Neighbors}
\label{fig:knn}
\end{figure}

\subsection*{Support Vector Machines}

Support vector machines (SVMs) are a set of related supervised learning methods which analyze data and recognize patterns, used for classification and regression. It is a binary classifier, but can be extended to be a multi class classifier by combining multiple binary classifiers. 

SVM uses a subset of the train data (the support vectors) that lay on the borders of a category cluster to calculate a decision boundary (DB). This DB is constructed by maximizing the distance between the support vectors two classes. The decision boundary, a linear function, is later used to classify new data points.

Often a space with two sets datapoint classes are not linearly separable. For this reason the the original space is mapped to a higher dimensional space making the separation easier. This is done by a kernel.

The training phase consists of computing a kernel to make the space linearly separable. Then a decision boundary is calculated. Training a SVM classifier can be very computationally expensive, but classification is very fast and memory efficient.

The evaluated kernels are the Radial Basis Function (RBF) and a precomputed kernel using the $\chi^2$ method on the train set. For the RBF kernel 2 parameters are important, the cost $c$ and $\gamma$. The optimal values of these variables for these experiments where found with a brute force grid search on a small cluster.

\autoref{fig:svm} is an example of a SVM classification setting. The space is already linearly separable, so no space transformation is required. 

\begin{figure}[htbp]
\center{}
\includegraphics[width=0.3\linewidth]{figures/svm.png}
\caption{Classification using Support Vector machines}
\label{fig:svm}
\end{figure}




\subsection*{The stabilizer}
\label{subsec:stabilizer}
Since the frames following each other have a spatial-temporal relationship there is a relationship between the labels given by the classifier. Because of noise, misclassification can happen. These false matches can be filtered out by smoothing the labels on a time scale.

This smoothing is done with a simple self invented method called 'the stabilizer'.

A visual representation is given in \autoref{fig:stabilizer}. The stabilizer is initialized with $n$ numbers of bins which is equal to the number of labels. There are 2 parameters, $V_{max}$ which controls the maximum and $V_{th}$ which controls the threshold.

For every new label that is given by the classifier all bins are decremented with 1, except for the bin with the currently classified label which is incremented. A bin is incremented until it reaches $V_{max}$. When at any moment one of the bins value rises above $V_{th}$, the stabilizer will output the label of that bin. The result is a more stable and smoothed stream of labels, where single noisy labels are filtered out. For a sequence of frames with a correctly detected pose there is a delay between the first frame and the stabilizer will output the correct label, this delay is controlled by $V_{th}$ which is measured in frame count. A higher value will reduce more noise, but will give a bigger delay. There is also a delay after a sequence which is controlled by $n_{max}$. With a framerate between $10$ and $25$ frames a second, $V_{max} = 15$ and $V_{th} = 10$ give good results reducing noise and still being very responsive.

\begin{figure}[htbp]
  \centering
\subfloat[A]{
    \includegraphics[width=0.7\textwidth]{figures/stabilizer/b.png}
}
\hspace{0.03\linewidth}
\subfloat[B]{
    \includegraphics[width=0.7\textwidth]{figures/stabilizer/a.png}
}
  \caption{Two examples of a stabilizer in action}
  \label{fig:stabilizer}
\end{figure}




\subsection*{Training phase}
recording video, manual labeling, extracting HOG features, training classifier

\section{Discussion}
